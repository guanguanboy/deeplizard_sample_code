{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch构建神经网络(三)(29-33节)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1&4.2 使用tensorboard可视化CNN训练指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pytorch1.1.0以上的版本已经自动增加了tensorboard\n",
    "* 在终端输入“tensorboard --version”可查看tensorboard的版本\n",
    "* 在终端输入“tensorboard --logdir=runs”进入tensorboard(在写了tensorboard数据的路径下)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa4b33078b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu101\n",
      "0.8.2+cu101\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = t \n",
    "        t = F.relu(self.conv1(t))\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t = F.relu(self.conv2(t))\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t = t.reshape(-1, 12*4*4)  # t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        \n",
    "        t = F.relu(self.fc2(t))\n",
    "        \n",
    "        t = self.out(t)\n",
    "        return t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb268dd8e7b54b1b8148f3859a2fdb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8012c7348d3b443586a0a1ec6373590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ff4acdca0a41fcaf00add9a07fd8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d494618ff5c4421b2c8f5a75e8080a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liguanlin/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "#train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting out with TensorBoard (Network Graph and Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 45290 loss 38179.45653498173\n",
      "epoch: 1 total_correct: 50721 loss 25094.803376495838\n",
      "epoch: 2 total_correct: 51537 loss 22882.45138823986\n",
      "epoch: 3 total_correct: 52071 loss 21670.569276809692\n",
      "epoch: 4 total_correct: 52107 loss 21282.01998323202\n",
      "epoch: 0 total_correct: 42664 loss 46462.98564076424\n",
      "epoch: 1 total_correct: 48768 loss 29874.165099859238\n",
      "epoch: 2 total_correct: 50565 loss 25865.337638556957\n",
      "epoch: 3 total_correct: 51469 loss 23272.86912202835\n",
      "epoch: 4 total_correct: 52161 loss 21535.871674120426\n",
      "epoch: 0 total_correct: 30376 loss 85042.43420958519\n",
      "epoch: 1 total_correct: 41851 loss 48603.57211828232\n",
      "epoch: 2 total_correct: 43668 loss 43475.605699419975\n",
      "epoch: 3 total_correct: 44742 loss 40351.34474039078\n",
      "epoch: 4 total_correct: 45570 loss 38223.656898736954\n",
      "epoch: 0 total_correct: 6438 loss 137509.3246459961\n",
      "epoch: 1 total_correct: 21843 loss 130312.067425251\n",
      "epoch: 2 total_correct: 29357 loss 107050.07047653198\n",
      "epoch: 3 total_correct: 32749 loss 82947.30160236359\n",
      "epoch: 4 total_correct: 35678 loss 70590.48555493355\n",
      "epoch: 0 total_correct: 36434 loss 59005.755007267\n",
      "epoch: 1 total_correct: 47938 loss 31564.80821967125\n",
      "epoch: 2 total_correct: 50422 loss 26013.57203722\n",
      "epoch: 3 total_correct: 51523 loss 22980.21399974823\n",
      "epoch: 4 total_correct: 52224 loss 20887.587755918503\n",
      "epoch: 0 total_correct: 27716 loss 95220.33673524857\n",
      "epoch: 1 total_correct: 43898 loss 42354.61312532425\n",
      "epoch: 2 total_correct: 45890 loss 36298.69121313095\n",
      "epoch: 3 total_correct: 47081 loss 33332.35511183739\n",
      "epoch: 4 total_correct: 47916 loss 31530.50997853279\n",
      "epoch: 0 total_correct: 7901 loss 137049.16429519653\n",
      "epoch: 1 total_correct: 23548 loss 127686.11061573029\n",
      "epoch: 2 total_correct: 32214 loss 97391.85631275177\n",
      "epoch: 3 total_correct: 37342 loss 68670.19420862198\n",
      "epoch: 4 total_correct: 39744 loss 57213.797986507416\n",
      "epoch: 0 total_correct: 6526 loss 138275.87985992432\n",
      "epoch: 1 total_correct: 6814 loss 138180.82880973816\n",
      "epoch: 2 total_correct: 8188 loss 138078.47666740417\n",
      "epoch: 3 total_correct: 11941 loss 137947.44420051575\n",
      "epoch: 4 total_correct: 12039 loss 137789.86930847168\n",
      "epoch: 0 total_correct: 16556 loss 126093.73450279236\n",
      "epoch: 1 total_correct: 26228 loss 87573.82154464722\n",
      "epoch: 2 total_correct: 34150 loss 66366.42098426819\n",
      "epoch: 3 total_correct: 39518 loss 54670.21942138672\n",
      "epoch: 4 total_correct: 42264 loss 47328.53591442108\n",
      "epoch: 0 total_correct: 6112 loss 137747.2949028015\n",
      "epoch: 1 total_correct: 14207 loss 134880.4497718811\n",
      "epoch: 2 total_correct: 23888 loss 126863.12437057495\n",
      "epoch: 3 total_correct: 31490 loss 108769.54317092896\n",
      "epoch: 4 total_correct: 35778 loss 84046.62013053894\n",
      "epoch: 0 total_correct: 6000 loss 138224.37286376953\n",
      "epoch: 1 total_correct: 6000 loss 138128.38315963745\n",
      "epoch: 2 total_correct: 6000 loss 138025.20275115967\n",
      "epoch: 3 total_correct: 6000 loss 137902.8081893921\n",
      "epoch: 4 total_correct: 6000 loss 137754.65726852417\n",
      "epoch: 0 total_correct: 6000 loss 138381.95085525513\n",
      "epoch: 1 total_correct: 6000 loss 138374.96519088745\n",
      "epoch: 2 total_correct: 6000 loss 138368.22271347046\n",
      "epoch: 3 total_correct: 5999 loss 138361.43016815186\n",
      "epoch: 4 total_correct: 6000 loss 138354.63523864746\n"
     ]
    }
   ],
   "source": [
    "#batch_size = 100\n",
    "#lr =0.01\n",
    "# 对不同的batchsize，lr的训练情况进行比较\n",
    "# 方法1：但此方法需要多层for循环\n",
    "batch_size_list = [100, 1000, 10000]\n",
    "lr_list = [.01, .001, .0001, .00001]\n",
    "for batch_size in batch_size_list:\n",
    "    for lr in lr_list:\n",
    "        network = Network()\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        images, labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)   # 创建能在tensorboard中查看的图像网格\n",
    "\n",
    "        comment = f'batch_size={batch_size} lr ={lr}'\n",
    "        tb = SummaryWriter(comment=comment)   # 在Summary Writer添加该注释，可帮助我们在tensorboard中唯一地识别该表示\n",
    "        tb.add_image('images', grid)  # 将一批图像放在grid中进行显示\n",
    "        tb.add_graph(network, images)   # 在tensorboard中看见网络结构的可视化图\n",
    "        optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(5):\n",
    "    \n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "    \n",
    "            for batch in train_loader:    # Get Batch\n",
    "                images, labels = batch\n",
    "        \n",
    "                preds = network(images) # Pass Batch\n",
    "                loss = F.cross_entropy(preds, labels)  # Calculate loss\n",
    "        \n",
    "                optimizer.zero_grad()    # 梯度清零，否则会累加\n",
    "                loss.backward()     # Calculate Gradients\n",
    "                optimizer.step()    # Update Weights\n",
    "        \n",
    "                #total_loss += loss.item()\n",
    "                total_loss += loss.item()*batch_size # 在对不同批次下的训练进行比较时，这样做可使结果更具有可比性\n",
    "                total_correct += get_num_correct(preds, labels)\n",
    "        \n",
    "            tb.add_scalar(\"Loss\", total_loss, epoch)\n",
    "            tb.add_scalar(\"Number Correct\", total_correct, epoch)\n",
    "            tb.add_scalar(\"Accuracy\", total_correct/len(train_set), epoch)\n",
    "            '''\n",
    "            这种表达方式只能看单个层的偏置，权重，及其梯度的变化趋势，无法看到全部的\n",
    "            tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "            tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "            tb.add_histogram('conv1.weight.grad', network.conv1.weight.grad, epoch)\n",
    "            '''\n",
    "            for name, weight in network.named_parameters():\n",
    "                tb.add_histogram(name, weight, epoch)\n",
    "                tb.add_histogram(f'{name}.grad', weight.grad, epoch)\n",
    "            print(\"epoch:\", epoch, \"total_correct:\", total_correct, \"loss\", total_loss)\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对多层的偏置，权重及其梯度进行访问的原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5])\n",
      "conv1.bias torch.Size([6])\n",
      "conv2.weight torch.Size([12, 6, 5, 5])\n",
      "conv2.bias torch.Size([12])\n",
      "fc1.weight torch.Size([120, 192])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([60, 120])\n",
      "fc2.bias torch.Size([60])\n",
      "out.weight torch.Size([10, 60])\n",
      "out.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,weight in network.named_parameters():\n",
    "    print(name, weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight.grad torch.Size([6, 1, 5, 5])\n",
      "conv1.bias.grad torch.Size([6])\n",
      "conv2.weight.grad torch.Size([12, 6, 5, 5])\n",
      "conv2.bias.grad torch.Size([12])\n",
      "fc1.weight.grad torch.Size([120, 192])\n",
      "fc1.bias.grad torch.Size([120])\n",
      "fc2.weight.grad torch.Size([60, 120])\n",
      "fc2.bias.grad torch.Size([60])\n",
      "out.weight.grad torch.Size([10, 60])\n",
      "out.bias.grad torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,weight in network.named_parameters():\n",
    "    print(f'{name}.grad', weight.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更简单的方法对要更改的参数进行访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    lr = [.01, .001],\n",
    "    batc_size = [10, 100, 1000],\n",
    "    shuffle = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.01, 0.001], [10, 100, 1000], [True, False]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_values = [v for v in parameters.values()]\n",
    "param_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 10 True\n",
      "0.01 10 False\n",
      "0.01 100 True\n",
      "0.01 100 False\n",
      "0.01 1000 True\n",
      "0.01 1000 False\n",
      "0.001 10 True\n",
      "0.001 10 False\n",
      "0.001 100 True\n",
      "0.001 100 False\n",
      "0.001 1000 True\n",
      "0.001 1000 False\n"
     ]
    }
   ],
   "source": [
    "for lr, batch_size, shuffle in product(*param_values):\n",
    "    print(lr, batch_size, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01, 0.001], [10, 100], [True, False]]\n",
      "epoch: 0 total_correct: 45917 loss 37714.80418500956\n",
      "epoch: 1 total_correct: 48133 loss 32530.364042751025\n",
      "epoch: 2 total_correct: 48350 loss 32202.91669322527\n",
      "epoch: 3 total_correct: 48445 loss 32556.72937942203\n",
      "epoch: 4 total_correct: 47708 loss 33675.98075509537\n",
      "epoch: 0 total_correct: 46237 loss 37775.58383287396\n",
      "epoch: 1 total_correct: 48467 loss 32216.82432834059\n",
      "epoch: 2 total_correct: 48816 loss 31358.76809327863\n",
      "epoch: 3 total_correct: 49056 loss 30401.91280476749\n",
      "epoch: 4 total_correct: 49312 loss 29917.737400811166\n",
      "epoch: 0 total_correct: 46716 loss 34790.05429893732\n",
      "epoch: 1 total_correct: 51398 loss 23303.143826127052\n",
      "epoch: 2 total_correct: 52157 loss 21170.998410880566\n",
      "epoch: 3 total_correct: 52476 loss 20294.516563415527\n",
      "epoch: 4 total_correct: 52640 loss 19811.726446449757\n",
      "epoch: 0 total_correct: 46889 loss 34496.610844135284\n",
      "epoch: 1 total_correct: 51559 loss 22754.824651777744\n",
      "epoch: 2 total_correct: 52217 loss 20874.746322631836\n",
      "epoch: 3 total_correct: 52579 loss 20092.032472789288\n",
      "epoch: 4 total_correct: 52892 loss 19183.51531177759\n",
      "epoch: 0 total_correct: 47683 loss 32934.95142668486\n",
      "epoch: 1 total_correct: 52144 loss 21262.18342244625\n",
      "epoch: 2 total_correct: 53010 loss 18731.328611165518\n",
      "epoch: 3 total_correct: 53595 loss 17224.20106755977\n",
      "epoch: 4 total_correct: 53959 loss 16244.330473375157\n",
      "epoch: 0 total_correct: 46686 loss 35400.47559546307\n",
      "epoch: 1 total_correct: 51362 loss 23511.057745181024\n",
      "epoch: 2 total_correct: 52400 loss 20561.16921280278\n",
      "epoch: 3 total_correct: 52956 loss 18968.554524696665\n",
      "epoch: 4 total_correct: 53357 loss 17699.63314777764\n",
      "epoch: 0 total_correct: 41948 loss 48963.55943083763\n",
      "epoch: 1 total_correct: 47967 loss 32345.82622051239\n",
      "epoch: 2 total_correct: 49813 loss 27965.415757894516\n",
      "epoch: 3 total_correct: 50895 loss 25031.691612303257\n",
      "epoch: 4 total_correct: 51650 loss 23147.837106883526\n",
      "epoch: 0 total_correct: 42719 loss 45869.294729828835\n",
      "epoch: 1 total_correct: 48591 loss 30523.00023138523\n",
      "epoch: 2 total_correct: 50455 loss 26471.889385581017\n",
      "epoch: 3 total_correct: 51332 loss 23956.702806055546\n",
      "epoch: 4 total_correct: 52050 loss 21995.64219713211\n"
     ]
    }
   ],
   "source": [
    "#batch_size = 100\n",
    "#lr =0.01\n",
    "# 对不同的batchsize，lr的训练情况进行比较\n",
    "# 方法2：只需一层循环\n",
    "from itertools import product\n",
    "parameters = dict(\n",
    "    lr = [.01, .001],\n",
    "    batch_size = [10, 100],\n",
    "    shuffle = [True, False]\n",
    ")\n",
    "param_values = [v for v in parameters.values()]\n",
    "print(param_values)\n",
    "for lr, batch_size, shuffle in product(*param_values):\n",
    "    network = Network()\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    images, labels = next(iter(train_loader))\n",
    "    grid = torchvision.utils.make_grid(images)   # 创建能在tensorboard中查看的图像网格\n",
    "\n",
    "    comment = f'batch_size={batch_size} lr ={lr} shuffle={shuffle}'\n",
    "    tb = SummaryWriter(comment=comment)   # 在Summary Writer添加该注释，可帮助我们在tensorboard中唯一地识别该表示\n",
    "    tb.add_image('images', grid)  # 将一批图像放在grid中进行显示\n",
    "    tb.add_graph(network, images)   # 在tensorboard中看见网络结构的可视化图\n",
    "    optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(5):\n",
    "    \n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "    \n",
    "        for batch in train_loader:    # Get Batch\n",
    "            images, labels = batch\n",
    "        \n",
    "            preds = network(images) # Pass Batch\n",
    "            loss = F.cross_entropy(preds, labels)  # Calculate loss\n",
    "        \n",
    "            optimizer.zero_grad()    # 梯度清零，否则会累加\n",
    "            loss.backward()     # Calculate Gradients\n",
    "            optimizer.step()    # Update Weights\n",
    "        \n",
    "            #total_loss += loss.item()\n",
    "            total_loss += loss.item()*batch_size # 在对不同批次下的训练进行比较时，这样做可使结果更具有可比性\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "        \n",
    "        tb.add_scalar(\"Loss\", total_loss, epoch)\n",
    "        tb.add_scalar(\"Number Correct\", total_correct, epoch)\n",
    "        tb.add_scalar(\"Accuracy\", total_correct/len(train_set), epoch)\n",
    "        '''\n",
    "            这种表达方式只能看单个层的偏置，权重，及其梯度的变化趋势，无法看到全部的\n",
    "            tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "            tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "            tb.add_histogram('conv1.weight.grad', network.conv1.weight.grad, epoch)\n",
    "        '''\n",
    "        for name, weight in network.named_parameters():\n",
    "            tb.add_histogram(name, weight, epoch)\n",
    "            tb.add_histogram(f'{name}.grad', weight.grad, epoch)\n",
    "        print(\"epoch:\", epoch, \"total_correct:\", total_correct, \"loss\", total_loss)\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 RunBuilder类的编写\n",
    "* 该类的编写允许我们使用不同的参数值生成多个运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "    lr = [.01, .001],\n",
    "    batch_size = [1000, 10000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Run(lr=0.01, batch_size=1000),\n",
       " Run(lr=0.01, batch_size=10000),\n",
       " Run(lr=0.001, batch_size=1000),\n",
       " Run(lr=0.001, batch_size=10000)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs = RunBuilder.get_runs(params)\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(lr=0.01, batch_size=1000) 0.01 1000\n",
      "Run(lr=0.01, batch_size=10000) 0.01 10000\n",
      "Run(lr=0.001, batch_size=1000) 0.001 1000\n",
      "Run(lr=0.001, batch_size=10000) 0.001 10000\n"
     ]
    }
   ],
   "source": [
    "for run in runs:\n",
    "    print(run, run.lr, run.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Run(lr=0.01, batch_size=1000)\n",
      "-Run(lr=0.01, batch_size=10000)\n",
      "-Run(lr=0.001, batch_size=1000)\n",
      "-Run(lr=0.001, batch_size=10000)\n"
     ]
    }
   ],
   "source": [
    "# 创建RunBuilder类以后，comment表示为：\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    comment = f'-{run}'\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 如何试验大量的超参数\n",
    "* 构建RunManager类可实现对大量超参数的试验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        \n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "        \n",
    "    def begin_run(self, run, network, loader):\n",
    "        self.run_start_time = time.time()\n",
    "        \n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "        \n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment=f'-{run}')\n",
    "        \n",
    "        images. labels = next(iter(self.loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "        \n",
    "        self.tb.add_image('images', grid)\n",
    "        self.tb.add_graph(self.network, images)\n",
    "        \n",
    "    def end_run(self):\n",
    "        self.tb.close()\n",
    "        self.epoch_count = 0\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "        \n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "    \n",
    "    def end_epoch(self):\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "        \n",
    "        loss = self.epoch_loss/len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct/len(self.loader.dataset)\n",
    "        \n",
    "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
    "        \n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(name, param, self.epoch_count)\n",
    "            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
    "            \n",
    "        results = OrderedDict()\n",
    "        results[\"run\"] = self.run_count\n",
    "        results[\"epoch\"] = self.epoch_count\n",
    "        results[\"loss\"] = loss\n",
    "        results[\"accuracy\"] = accuracy\n",
    "        results[\"epoch duration\"] = epoch_duration\n",
    "        results[\"run duration\"] = run_duration\n",
    "        for k,v in self.run_params._asdict().items(): results[k] = v\n",
    "        self.run_data.append(results)\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "    \n",
    "    def track_loss(self, loss):\n",
    "        self.epoch_loss += loss.item()*self.loader.batch_size\n",
    "    \n",
    "    def track_num_correct(self, preds, labels):\n",
    "        self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _get_num_correct(self, preds, labels):\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "    \n",
    "    def save(self, fileName):\n",
    "        pd.DataFrame.from_dict(\n",
    "            self.run_data,\n",
    "            orient='columns').to_csv(f'{fileName}.csv')\n",
    "        with open(f'{fileName},json','w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>shuffle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.042737</td>\n",
       "      <td>0.590383</td>\n",
       "      <td>8.256754</td>\n",
       "      <td>8.816769</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.559194</td>\n",
       "      <td>0.781400</td>\n",
       "      <td>8.143764</td>\n",
       "      <td>17.074102</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.464315</td>\n",
       "      <td>0.825433</td>\n",
       "      <td>8.609629</td>\n",
       "      <td>25.794387</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.399532</td>\n",
       "      <td>0.852483</td>\n",
       "      <td>7.797880</td>\n",
       "      <td>33.656303</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.366366</td>\n",
       "      <td>0.865367</td>\n",
       "      <td>8.048990</td>\n",
       "      <td>41.774549</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.032360</td>\n",
       "      <td>0.602850</td>\n",
       "      <td>7.882747</td>\n",
       "      <td>8.481362</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.571882</td>\n",
       "      <td>0.777617</td>\n",
       "      <td>7.882565</td>\n",
       "      <td>16.467835</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.465295</td>\n",
       "      <td>0.826700</td>\n",
       "      <td>8.040931</td>\n",
       "      <td>24.586972</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.401143</td>\n",
       "      <td>0.851350</td>\n",
       "      <td>7.557076</td>\n",
       "      <td>32.232198</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.364107</td>\n",
       "      <td>0.865650</td>\n",
       "      <td>8.374050</td>\n",
       "      <td>40.727667</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.414950</td>\n",
       "      <td>0.479533</td>\n",
       "      <td>8.104691</td>\n",
       "      <td>8.693954</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.709832</td>\n",
       "      <td>0.724967</td>\n",
       "      <td>8.173051</td>\n",
       "      <td>16.949204</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.605531</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>7.955910</td>\n",
       "      <td>25.014194</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.531843</td>\n",
       "      <td>0.796950</td>\n",
       "      <td>9.066611</td>\n",
       "      <td>34.196472</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.486776</td>\n",
       "      <td>0.819500</td>\n",
       "      <td>8.388909</td>\n",
       "      <td>42.692436</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.411931</td>\n",
       "      <td>0.477067</td>\n",
       "      <td>8.906703</td>\n",
       "      <td>9.987738</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.695924</td>\n",
       "      <td>0.733433</td>\n",
       "      <td>7.838515</td>\n",
       "      <td>17.932502</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.568650</td>\n",
       "      <td>0.778983</td>\n",
       "      <td>8.436254</td>\n",
       "      <td>26.490002</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.506951</td>\n",
       "      <td>0.807350</td>\n",
       "      <td>7.863464</td>\n",
       "      <td>34.473862</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.465133</td>\n",
       "      <td>0.831083</td>\n",
       "      <td>8.481223</td>\n",
       "      <td>43.070768</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    run  epoch      loss  accuracy  epoch duration  run duration    lr  \\\n",
       "0     1      1  1.042737  0.590383        8.256754      8.816769  0.01   \n",
       "1     1      2  0.559194  0.781400        8.143764     17.074102  0.01   \n",
       "2     1      3  0.464315  0.825433        8.609629     25.794387  0.01   \n",
       "3     1      4  0.399532  0.852483        7.797880     33.656303  0.01   \n",
       "4     1      5  0.366366  0.865367        8.048990     41.774549  0.01   \n",
       "5     2      1  1.032360  0.602850        7.882747      8.481362  0.01   \n",
       "6     2      2  0.571882  0.777617        7.882565     16.467835  0.01   \n",
       "7     2      3  0.465295  0.826700        8.040931     24.586972  0.01   \n",
       "8     2      4  0.401143  0.851350        7.557076     32.232198  0.01   \n",
       "9     2      5  0.364107  0.865650        8.374050     40.727667  0.01   \n",
       "10    3      1  1.414950  0.479533        8.104691      8.693954  0.01   \n",
       "11    3      2  0.709832  0.724967        8.173051     16.949204  0.01   \n",
       "12    3      3  0.605531  0.764000        7.955910     25.014194  0.01   \n",
       "13    3      4  0.531843  0.796950        9.066611     34.196472  0.01   \n",
       "14    3      5  0.486776  0.819500        8.388909     42.692436  0.01   \n",
       "15    4      1  1.411931  0.477067        8.906703      9.987738  0.01   \n",
       "16    4      2  0.695924  0.733433        7.838515     17.932502  0.01   \n",
       "17    4      3  0.568650  0.778983        8.436254     26.490002  0.01   \n",
       "18    4      4  0.506951  0.807350        7.863464     34.473862  0.01   \n",
       "19    4      5  0.465133  0.831083        8.481223     43.070768  0.01   \n",
       "\n",
       "    batch_size  shuffle  \n",
       "0         1000     True  \n",
       "1         1000     True  \n",
       "2         1000     True  \n",
       "3         1000     True  \n",
       "4         1000     True  \n",
       "5         1000    False  \n",
       "6         1000    False  \n",
       "7         1000    False  \n",
       "8         1000    False  \n",
       "9         1000    False  \n",
       "10        2000     True  \n",
       "11        2000     True  \n",
       "12        2000     True  \n",
       "13        2000     True  \n",
       "14        2000     True  \n",
       "15        2000    False  \n",
       "16        2000    False  \n",
       "17        2000    False  \n",
       "18        2000    False  \n",
       "19        2000    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a2acb8f7f809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resuls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-1288498d0545>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fileName)\u001b[0m\n\u001b[1;32m     90\u001b[0m             orient='columns').to_csv(f'{fileName}.csv')\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{fileName},json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 使用RunManager和RunBuilder类可以使得程序更易扩展\n",
    "params = OrderedDict(\n",
    "    lr = [.01],\n",
    "    batch_size =[1000, 2000],\n",
    "    shuffle = [True, False]\n",
    ")\n",
    "m = RunManager()\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    network = Network()\n",
    "    loader = DataLoader(train_set, batch_size=run.batch_size, shuffle=run.shuffle)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "    \n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(5):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            images, labels = batch\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            m.track_loss(loss)\n",
    "            m.track_num_correct(preds, labels)\n",
    "            \n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save('resuls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 使用DataLoader的多进程功能加速神经网络训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用data loader类的num_workers可选属性可加速神经网络的训练\n",
    "* num_workers属性告诉data loader实例有多少个单元处理器用于数据加载\n",
    "* num_workers值的选择的最好方式是进行试验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用RunManager和RunBuilder类可以使得程序更易扩展\n",
    "params = OrderedDict(\n",
    "    lr = [.01],\n",
    "    batch_size =[1000, 2000],\n",
    "    shuffle = [True, False]，\n",
    "    num_workers = [0,1,2,4,8,16]\n",
    ")\n",
    "m = RunManager()\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    \n",
    "    network = Network()\n",
    "    loader = DataLoader(train_set, batch_size=run.batch_size, shuffle=run.shuffle, num_workers=run.num_workers)\n",
    "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
    "    \n",
    "    m.begin_run(run, network, loader)\n",
    "    for epoch in range(5):\n",
    "        m.begin_epoch()\n",
    "        for batch in loader:\n",
    "            images, labels = batch\n",
    "            preds = network(images)\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            m.track_loss(loss)\n",
    "            m.track_num_correct(preds, labels)\n",
    "            \n",
    "            \n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save('resuls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
